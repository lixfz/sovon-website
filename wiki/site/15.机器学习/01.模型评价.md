[TOC]

---
### 混淆矩阵

<div>
<table  class="table table-primary align-middle table-bordered">
<tbody>
  <tr>
    <td rowspan="2" colspan="2">Confusion Matrix</td> 
    <td colspan="2">真实情况</td> 
  </tr>
  <tr>      
    <td>正</td>
    <td>负</td>
  </tr>
  <tr>
    <td rowspan="2">预测结果</td>
    <td>正 Positive</td>
    <td class="table-success">真阳性 TP</td>
    <td class="table-success">伪阳性 FP</td>
  </tr>
  <tr> 
    <td>负 Negative</td>
    <td class="table-success">伪阴性 FN</td>
    <td class="table-success">真阴性 TN</td>
  </tr>
</tbody>
</table> 
</div>


* 真阳性TP (True Positive)：将正类预测为正类数
* 伪阳性FP (False Positive)：将负类预测为正类数
* 伪阴性FN (False Negative)：将正类预测为负类数
* 真阴性TN (True Negative)：将负类预测为负类数



### 准确率 Accuracy

即：就是所有的预测正确（正类负类）的占总的比重。  公式：

```math
Accuracy=\frac{tp+tn}{tp+tn+fp+fn}
```

### 精确率 Precision

查准率，即正确预测为正的占全部预测为正的比例。 公式：
```math
Precision=\frac{tp}{tp+fp}
```

### 召回率 Recall

查全率，即正确预测为正的占全部实际为正的比例。公式：
```math
Recall = \frac{tp}{tp+fn}
```

### F1 score（H-mean）

F1值为算数平均数除以几何平均数，且越大越好，将Precision和Recall的上述公式带入会发现，当F1值小时，True  Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。
公式：

```math
\frac{2}{F_1}=\frac{1}{Precision}+\frac{1}{Recall}
```
转换之后：
```math
F_1 =\frac{2*Precision*Recall}{Precision+Recall} =\frac{2TP}{2TP+FP+FN}
```

### ROC曲线

ROC空间将伪阳性率（FPR）定义为 X 轴，真阳性率（TPR）定义为 Y 轴。 其中：

```math
TPR = \frac{TP}{TP+FN} ,  FPR=\frac{FP}{FP+PN}
```

给定一个二元分类模型和它的阈值，就能从所有样本的（阳性／阴性）真实值和预测值计算出一个 (X=FPR, Y=TPR) 座标点。从 (0, 0) 到 (1,1) 的对角线将ROC空间划分为左上／右下两个区域，在这条线的以上的点代表了一个好的分类结果（胜过随机分类），而在这条线以下的点代表了差的分类结果（劣于随机分类）。将同一模型每个阈值 的 (FPR, TPR) 座标都画在ROC空间里，就成为特定模型的ROC曲线。可以调整阈值的高低（将左上图的B垂直线往左或右移动），便会得出不同的伪阳性率与真阳性率，总之即得出不同的预测准确率。 

![img from wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6b/Roccurves.png/440px-Roccurves.png)

###  曲线下面积AUC

AUC即Area under the Curve of ROC (AUC ROC)，在比较不同的分类模型时，可以将每个模型的ROC曲线都画出来，比较曲线下面积做为模型优劣的指标。多数情况下，AUC值越大的分类器，正确率越高。

